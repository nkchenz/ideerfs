=object=

class Object(OODict):
    def __init__(self, data):
        self.id = next_seq()
        self.checksum = ''
        self.checksum_algo = 'sha1'
        self.version = ''
        self.ref = 0
        self.rf = 3   # replication_factor
        self.compress = ''
        self.type = ''
        
    def serialize()
        # store, first compress then checksum, then store to disk 
        # Save (type, compress, checksum, version, data) to file id
        pass

    def deserialize():
        # load
        pass

ChunkObject
DirObject
FileObject

object id is unique in object set.  
def next_seq():
    global seq
    seq += 1
    return seq
    
path hash maybe very useful to dht meta, but it's complicated when you want to rename
or move files
/a/b/c/file -> hash

data partition can be based on path. combine path hash and object id?

off some load of global id generating
lustre id: 如果object id有40位，storage manager可以向client哈哈发放30位的seqnumber，client
可以自己生成低10位，合在一起构成一个全局有效的id。


locations about which object is stored on which device is generated dynamically

object cow: 如果不修改原有object，就不需要version。只有当修改原对象时，才需要version以示区别。
when using copy-on-write, be sure to modify all the reference pointers to the new one

object splitting? file into chunks, what about dir? 统一的object模型，只有当object太大
的时候，才分割为chunk. B+ tree



=snapshot=

基于COW的snapshot算法只有在仅存在单向链接时才可用。

   a a'       
 b  c c'
     d d'

如果d变为d'，则a变为a'。如果b和a之间存在双向链接，b原来指向a，则b是否应该修改为指向a'？
这样会引起递归修改雪崩效应。


how does zfs handle '..' while snapshoting?
   a a'
  b c c'

a' and a has same object id, cow is only based on block level, and each block has
no backtracking pointer to upper block, so it's totally feasiable.

BLOCK LEVEL COW
ZFS uses a copy-on-write, transactional object model. All block pointers within 
the filesystem contain a 256-bit checksum of the target block which is verified 
when the block is read. Blocks containing active data are never overwritten in 
place; instead, a new block is allocated, modified data is written to it, and 
then any metadata blocks referencing it are similarly read, reallocated, and 
written. To reduce the overhead of this process, multiple updates are grouped 
into transaction groups, and an intent log is used when synchronous write 
semantics are required.
The ZFS copy-on-write model has another powerful advantage: when ZFS writes new 
data, instead of releasing the blocks containing the old data, it can instead 
retain them, creating a snapshot version of the file system. ZFS snapshots are 
created very quickly, since all the data comprising the snapshot is already 
stored; they are also space efficient, since any unchanged data is shared among
 the file system and its snapshots.
Writable snapshots ("clones") can also be created, resulting in two independent
 file systems that share a set of blocks. As changes are made to any of the clone
  file systems, new data blocks are created to reflect those changes, but any 
  unchanged blocks continue to be shared, no matter how many clones exist. 

=version tree which has backtracking pointers=

R1 <- R2 <- R3 <- R4

 A1 <- A2 <- A3 <- A4
  ^                  ^
B   C1     <-      C2


R is the root, which has a dir A, which has dir B, C.  New versions
point to old, so old ones can be left untouched.
'parent' field in each version always point to the oldest version of
its parent, ex. C1 is born with A1, C2 is born with A3.

And another super object S points to the latest root, in this case S=R4.


1. parent '..' works fine
from R2/A2/C1/..  we get A2
from R1/A1/C1/..  we get A1

2. find objects which reference a given object directly. in this case,
C1 is referenced by A2, A1

key point is find the next version C2 which points to C1. start from
super object S, find the latest Cn, then it's easy to get C2.
Problem may be complicated when some parents of C has been deleted at
the newer versions, but it still possible to find it, just search the
next older version at the level which does not contain the desired
child entry.


3. delete object

Suppose R1, A1 are freed,  shall we free C1?  It all depends on step2,
if the only object reference C1 is A1, then just free it, or else
modify C1.parent points to A2, which is just the oldest version of its
parent now. Though we are always trying to not modify object, but here
the modification of C1.parent seems unavoidable anyway.


= meta-chunk model =
This is different from 'everything is a object' model, it's the current implemention.
3 layers Model:
    Meta
    Storage
    Chunk

meta layer is only about files and dirs, storage is layer care about placement, 
and  replications, reference count, chunk layer is mainly the memory.
 
client may interact with any of this layer regarding to each operations 

storage layer know nothing about which dir contains which file, and which file contains
which chunk, so when you delete dir recursively, you'd better compute all the chunks
should be deleted at meta layer. You can't just give storage layer a dir or file id,
this violates the rules above.
object ref has nothing to do with object type, only storage layer care about ref count,
shall have a interface to manage it
in fact, meta object like dir and file object type shall has their own storage layer.
it's better if we can unify the two storage layer for meta and chunk.



= milestone =

v0
princple version
-no transactions
-no locks
-no use then
-one meta node, many chunk nodes

v1
-seq chunk write, do not support stride
-only replicate chunk
-chunk is named under fid
-meta rw is different from chunk rw

-single fs, single dataset and single storage pool
-1w loc

v2

v3
-everything is a object
-every object has a replication factor
-cow, easy snapshot
-chunk_meta is stored under Meta on meta server, chunk_data is stored under Object
 on chunk servers
 
v4
dht meta, clustre meta
multifs support
data partition
object db
re-balancing chunk nodes
cache consistency
notify list


=TODO=
- space management
- delete
- client buffer
- hashlist
- *** journal and recovery
Why doesn't ext3 get hit by this?  Well, because ext3 does physical
block journalling.  This means that we write the entire physical block
to the journal and, only have the updates to the journal are commited,
do we write the data to the final location on disk.  So if you yank
out the power cord, and inode tables get trashed, they will get
restored when the journal gets replayed.
- mapreduce
- heart beat message
- lookaside table cache
