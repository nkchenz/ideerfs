=object=

class Object(OODict):
    def __init__(self, data):
        self.id = next_seq()
        self.checksum = ''
        self.checksum_algo = 'sha1'
        self.version = ''
        self.ref = 0
        self.rf = 3   # replication_factor
        self.compress = ''
        self.type = ''

    def serialize()
        # store, first compress then checksum, then store to disk
        # Save (type, compress, checksum, version, data) to file id
        pass

    def deserialize():
        # load
        pass

ChunkObject
DirObject
FileObject

object id is unique in object set.
def next_seq():
    global seq
    seq += 1
    return seq

path hash maybe very useful to dht meta, but it's complicated when you want to rename
or move files
/a/b/c/file -> hash

data partition can be based on path. combine path hash and object id?

off some load of global id generating
lustre id: 如果object id有40位，storage manager可以向client发放30位的seqnumber，client可以自己生成低10位，合在一起构成全局唯一的id


每个node启动的时候，加入集群，向master注册自己拥有的对象。
locations about which object is stored on which device is generated dynamically

如果不修改原有object，就不需要version。只有当修改原对象时，才需要version以示区别。

when using copy-on-write, be sure to modify all the reference pointers to the new one

统一的object模型，只有当object太大才分割为chunk. B+ tree。chunk只是一个存储策略，object dir and object 才是抽象意义上一致性。

=snapshot=

基于COW的snapshot算法只有在仅存在单向链接时才可用。

   a a'
 b  c c'
     d d'

如果d变为d'，则a变为a'。如果b和a之间存在双向链接，b原来指向a，则b是否应该修改为指向a'？
这样会引起递归修改雪崩效应。


how does zfs handle '..' while snapshoting?
   a a'
  b c c'

a' and a has same object id, cow is only based on block level, and each block has
no backtracking pointer to upper block, so it's totally feasiable.

BLOCK LEVEL COW
ZFS uses a copy-on-write, transactional object model. All block pointers within
the filesystem contain a 256-bit checksum of the target block which is verified
when the block is read. Blocks containing active data are never overwritten in
place; instead, a new block is allocated, modified data is written to it, and
then any metadata blocks referencing it are similarly read, reallocated, and
written. To reduce the overhead of this process, multiple updates are grouped
into transaction groups, and an intent log is used when synchronous write
semantics are required.
The ZFS copy-on-write model has another powerful advantage: when ZFS writes new
data, instead of releasing the blocks containing the old data, it can instead
retain them, creating a snapshot version of the file system. ZFS snapshots are
created very quickly, since all the data comprising the snapshot is already
stored; they are also space efficient, since any unchanged data is shared among
 the file system and its snapshots.
Writable snapshots ("clones") can also be created, resulting in two independent
 file systems that share a set of blocks. As changes are made to any of the clone
  file systems, new data blocks are created to reflect those changes, but any
  unchanged blocks continue to be shared, no matter how many clones exist.

=version tree which has backtracking pointers=

R1 <- R2 <- R3 <- R4

 A1 <- A2 <- A3 <- A4
  ^                  ^
B   C1     <-      C2


R is the root, which has a dir A, which has dir B, C.  New versions
point to old, so old ones can be left untouched.
'parent' field in each version always point to the oldest version of
its parent, ex. C1 is born with A1, C2 is born with A3.

And another super object S points to the latest root, in this case S=R4.


1. parent '..' works fine
from R2/A2/C1/..  we get A2
from R1/A1/C1/..  we get A1

2. find objects which reference a given object directly. in this case,
C1 is referenced by A2, A1

key point is find the next version C2 which points to C1. start from
super object S, find the latest Cn, then it's easy to get C2.
Problem may be complicated when some parents of C has been deleted at
the newer versions, but it still possible to find it, just search the
next older version at the level which does not contain the desired
child entry.


3. delete object

Suppose R1, A1 are freed,  shall we free C1?  It all depends on step2,
if the only object reference C1 is A1, then just free it, or else
modify C1.parent points to A2, which is just the oldest version of its
parent now. Though we are always trying to not modify object, but here
the modification of C1.parent seems unavoidable anyway.


= meta-chunk model =
This is different from 'everything is a object' model, it's the current implemention.
3 layers Model:
    Meta
    Storage
    Chunk

meta layer is only about files and dirs, storage is layer care about placement,
and  replications, reference count, chunk layer is mainly the memory.

client may interact with any of this layer regarding to each operations

storage layer know nothing about which dir contains which file, and which file contains
which chunk, so when you delete dir recursively, you'd better compute all the chunks
should be deleted at meta layer. You can't just give storage layer a dir or file id,
this violates the rules above.
object ref has nothing to do with object type, only storage layer care about ref count,
shall have a interface to manage it
in fact, meta object like dir and file object type shall has their own storage layer.
it's better if we can unify the two storage layer for meta and chunk.



=journal=
Why doesn't ext3 get hit by this?  Well, because ext3 does physical
block journalling.  This means that we write the entire physical block
to the journal and, only have the updates to the journal are commited,
do we write the data to the final location on disk.  So if you yank
out the power cord, and inode tables get trashed, they will get
restored when the journal gets replayed.

For this reason, the Namenode can be configured to support maintaining multiple copies of the FsImage and EditLog. Any update
to either the FsImage or EditLog causes each of the FsImages and EditLogs to get updated
synchronously.


=services=
maybe chunk, meta, storage services should run on different ports, or we shall start server first, then register service. Now, the situation is that even if we have instantiated one service class, server for it is not started yet really. Becareful.


concurrent write mutation successes  consistent but undefined
record append                        defined but inconsistent

append: offset is choosen by the filesystem, to guarantee data to be appended atomically at least once, which is cared about by application, offset offen does not matter.

dlm

path-hash: GFS logically represents its namespace as a lookup table mapping
full pathnames to metadata. 针对path的多层lock，可以同时修改一个目录内的文件，因为每个文件对应不同的path lock。


A file region is consistent if all clients will always see the same data, regardless of which replicas they read from. A region is defined after a file data mutation if it
is consistent and clients will see what the mutation writes in its entirety.

append操作如果任何一个replication失败，则重试整个操作。所以有的replication可能含有重复的record。 不保证所有的chunk都是bitwise一致的。但是保证操作成功时的写入区间所有replications都是一致的而且原子性的，defined。其他区间可能不一致。
1: AA0B
2: 0ABB
有效数据只是第2个记录A和第4个记录B。 第一次写入A在chunk2丢失，所以又重新在所有chunk写入，文件偏移已经发生了变化。第三次写入B在chunk1丢失。该文件在记录2，4是defined，记录1，3是不一致的。保证记录不会丢失，但并不能保证不会重复，应用程序自己应该负责检测记录是否重复。对于数据库应用可以使用主键。别的应用可以自定义一个记录id。


self-validating self-identity records

checkpoint of writing 日志，不用replay太多的journal

grant chunk lease to primary replication, which picks a serial order for applying mutaions.

decouple data flow of chunk piping with control flow

concurrent writes maybe overwritten or interleaved with, consistent but undefined, garbge

forward chunk data to the closest that has not received it: replica有master，slave之分，写master，读slave.  paxos协议，先提出comit请求，等多数节点同意后，在真正commit


同步还是异步？
一对进程之间的消息交换是有序的：消息被接收的顺序和被发送的顺序相同

请求如果过一定时间仍然没有收到响应，则可以认为是超时
some intelligent evolution of the cluster itself, software update automatically
master node: fault tolerant, auto restart services and nodes



=Notes=
1. english comment
2. small module
3. doc, doc, doc
4. code style, name style

useful file header
more consistent code style, and less ad-hoc 
cleanup module structure, misc, util and imports
document string, tell exactly what your code does


=Refs=
data sharding

meta cow vs. journal

ext3 journal
http://en.wikipedia.org/wiki/Ext3

journal fs
http://en.wikipedia.org/wiki/Journaling_file_system

anatomy of linux journal fs
http://download.boulder.ibm.com/ibmdl/pub/software/dw/linux/l-journaling-filesystems/l-journaling-filesystems-pdf.pdf

object model, journal, paxos protocal and chubby replica
